{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Name:-Yash solanki\n",
        "\n",
        "Reg email id:-yash87015@gmail.com\n",
        "\n",
        "Course name:-Data analytic\n",
        "\n",
        "Assignment name:-Statistics Advance - 1 Assignment\n",
        "\n",
        "Submission date:-3/11/2024\n",
        "\n",
        "Git link:-https://github.com/Yash87015/YASH (in git hub use assignment folder )"
      ],
      "metadata": {
        "id": "GE0yPI58fyiE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Explain the properties of the F-distribution.\n",
        "### Ans\n",
        "**Defination of f-dist**\n",
        "The f distribution is a probability distribution that is useful in the context of comparing variance of two or more sample\n",
        "\n",
        "**Properties of f-distribution**\n",
        "→ It is right skewed and takes only non-negative value because of the  s1^2& s2^2 .\n",
        "F = $\\frac{\\frac{s_1^2}{d_1}}{\\frac{s_2^2}{d_2}}\n",
        "$ .\n",
        "\n",
        "→ where s1 and s2 are independent random variable with chi-square distribution\n",
        "\n",
        "→ The F distribution has two degrees of freedom, one for the numerator and one for the denominator.\n",
        "\n",
        "→where d1&d2 is degree of freedom.the numerator degrees of freedom (d1) and the denominator degrees of freedom (d2).\n",
        "\n",
        "→ The F-test can only be used if both populations are normally distributed and independent. It is not robust, meaning it may produce unreliable results if the necessary conditions are not met.\n",
        "\n",
        "→ The F-ratio is close to one if the two populations have equal variances. If the F-ratio is much larger than one, the evidence is against the null hypothesis.\n",
        "\n",
        "→ As degree of freedom 1 & degree of freedom 2 is $\\geq$ 30 , the f-distribution beahave approximately like a normal distribution\n",
        "\n",
        "→ Mean and Variance: The mean of an F-distribution is approximately 1 for large sample sizes, and the variance depends on the degrees of freedom.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3xG_zPk5f9oa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. In which types of statistical tests is the F-distribution used, and why is it appropriate for these tests?\n",
        "\n",
        "### Ans\n",
        "The F-distribution is used in statistical tests to compare data from two or more populations\n",
        "\n",
        "(1)Analysis of variance\n",
        "→ Used to test if three or more samples come from populations with the same mean. ANOVA uses the F-distribution to determine if the observed differences between sample means are statistically significant.\n",
        "\n",
        "→ Why F-distribution: ANOVA compares the variance between group means to the variance within groups. The F-statistic helps determine if the variation among group means is greater than what would be expected by chance.\n",
        "\n",
        "(2)\tF-test\n",
        "→ Used to compare the variances of two samples or populations. The F-test uses the F-statistic to compare the variances by dividing them. This  test purpose  is two population variances are equal or not.\n",
        "\n",
        "→ Why F-distribution: The ratio of two sample variances (each divided by their respective degrees of freedom) follows an F-distribution if the samples are drawn from normally distributed populations.\n",
        "\n",
        "(3)\tRegression Analysis\n",
        "→ In multiple regression , the f-test assesses the overall signification of the model.\n",
        "\n",
        "→ Why F-distribution: It compares the variance explained by the regression model to the variance unexplained (error). A higher F-value indicates that the model explains a significant portion of the variability in the response variable.\n",
        "\n",
        "(4) Chow test\n",
        "Used to decide if there is a structural break in the data. The Chow test uses the F-test to determine if model 2 gives a significantly better fit to the data than model 1\n",
        "\n",
        " →  The F-distribution is appropriate when the populations from which the samples are drawn are normally distributed.\n",
        "\n",
        " → These characteristics make the F-distribution a valuable tool in various statistical analyses that involve variance comparisons and model evaluation.\n"
      ],
      "metadata": {
        "id": "m16CmXpx3S1u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. What are the key assumptions required for conducting an F-test to compare the variances of two populations?\n",
        "### Ans\n",
        "\n",
        "**Key assumption reuired for an f-test**\n",
        "\n",
        "→  The samples are independent of each other.\n",
        "\n",
        "→  The F-distribution assumes that the variances of the populations are equal when conducting ANOVA (homogeneity of variance).\n",
        "\n",
        "→ The data in both populations should be approximately normally distributed. This is particularly important for smaller sample sizes, as the F-test is sensitive to deviations from normality.(in short the samples are normally distributed)\n",
        "\n",
        "→ The data should be measured on an interval or ratio scale, allowing for meaningful calculations of variance.\n",
        "\n",
        "→ The samples should be randomly selected from their respective populations. This ensures that the samples are representative and helps to generalize the results.\n",
        "\n"
      ],
      "metadata": {
        "id": "xjjxmjYDFniD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. What is the purpose of ANOVA, and how does it differ from a t-test?\n",
        "### Ans\n",
        "Definition of ANOVA:-ANOVA is a statistical method used to compare the means of two or more groups\n",
        "\n",
        "→ Using variance you want to determine if the mean value of two or more group are same or different.\n",
        "\n",
        "→ **Assumption of ANOVA**:-(1)The population from which sample are drown should be normally distributed.\n",
        "(2)The sample should be independent of each other (Random)\n",
        "(3) Absence of outlier\n",
        "(4) homogeneity of variance:- Homogeneity meany that the variance among the groups should be approximately equal.\n",
        "\n",
        "→ **The purpose of ANOVA (analysis of variance) is to determine if there are statistically significant differences between three or more groups.**\n",
        "\n",
        "**Differnce between ANOVA and T-test**\n",
        "→ Both ANOVA and t-tests are used when samples are independent of each other, have approximately normal distributions, and have a high sample number. However, they determine statistical significance in different ways.\n",
        "\n",
        "→ t-test is statistical hypothesis test used to compare the means of two population groups while ANOVA is an observable technique used to compare the means of more than two population groups.\n",
        "\n",
        "→ ANOVA is a regression model with multiple covariates, while a t-test is a simple regression model with the covariate taking on only two values.\n",
        "\n",
        "\n",
        "→ t-test is used when the population is less than 30 but ANOVA is used for huge population counts.\n",
        "\n",
        "\n",
        "**Table of differnce between ANOVA and t-test**\n",
        "\n",
        "\n",
        "| Feature                     | ANOVA                               | t-test                              |\n",
        "|-----------------------------|-------------------------------------|-------------------------------------|\n",
        "| **Number of Groups**        | Three or more                       | Two                                 |\n",
        "| **Hypothesis Tested**       | Tests if all group means are equal | Tests if two group means are equal  |\n",
        "| **Post-Hoc Analysis**       | Often requires post-hoc tests      | No post-hoc analysis needed         |\n",
        "| **Type I Error Risk**       | Lower risk with multiple comparisons | Higher risk with multiple tests      |\n",
        "| **Assumptions**             | Normality, independence, homogeneity of variances | Normality and independence         |\n",
        "| **Output**                  | F-statistic and p-value            | t-statistic and p-value             |\n"
      ],
      "metadata": {
        "id": "8omlE-t0H3Rz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Explain when and why you would use a one-way ANOVA instead of multiple t-tests when comparing more than two groups.\n",
        "\n",
        "### Ans\n",
        "1. A one-way Analysis of Variance (ANOVA) is used instead of multiple t-tests when comparing more than two groups to avoid increasing the chance of error:\n",
        "\n",
        "  → Each time a t-test is conducted, there is a 5% chance of making a Type I error. When multiple t-tests are conducted, the chance of error increases. For example, if two t-tests are conducted, the chance of making a Type I error is 10%. ANOVA controls for these errors so that the Type I error remains at 5%(in other world Conducting multiple t-tests increases the risk of Type I error (false positives). For example, if you perform three t-tests, each at a significance level of 0.05, the overall probability of making at least one Type I error increases. One-way ANOVA controls this error rate by testing all groups simultaneously.)\n",
        "\n",
        "2. Overall Comparison:-One-way ANOVA provides a single test statistic (F-statistic) that assesses whether there are any significant differences among the group means. This is more efficient than conducting multiple t-tests.\n",
        "\n",
        "3. One-way ANOVA assumes that the variances among the groups are equal (homoscedasticity), which is an important consideration. While t-tests also assume this, ANOVA is specifically designed to evaluate this assumption across multiple groups.\n",
        "\n",
        "4. Using one statistical test (ANOVA) to assess the differences among multiple groups is more efficient in terms of both computation and interpretation. It reduces the complexity involved in analyzing multiple pairwise comparisons.\n",
        "\n",
        "→ **Conclusion:-** After studying the above differences, we can safely say that t-test is a special type of Analysis of Variance which is used when we only have two population means to compare. Hence, to avoid an increase in error while using a t-test to compare more than two population groups, we use ANOVA.\n"
      ],
      "metadata": {
        "id": "HXFA5OjmSacU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Explain how variance is partitioned in ANOVA into between-group variance and within-group variance.How does this partitioning contribute to the calculation of the F-statistic?\n",
        "\n",
        "### Ans\n",
        "In ANOVA, variance is partitioned into two components: **between-group variance** and **within-group variance**. This partitioning is crucial for understanding how group means differ and is essential for calculating the F-statistic.\n",
        "\n",
        " 1. **Between-Group Variance (BGV)**:\n",
        "- **Definition**: This component measures the variability among the means of different groups. It reflects how much the group means deviate from the overall mean.(Measures how different the group means are from each other )\n",
        "\n",
        "→ Measures how different the means of different groups are from each other.\n",
        "\n",
        "\n",
        " **Calculation**:\n",
        "$\n",
        "[\n",
        "\\text{Between-Group Sum of Squares (SSB)} = \\sum n_k (\\bar{X}_k - \\bar{X})^2\n",
        "]$\n",
        "\n",
        "where $(n_k)$ is the number of observations in group $(k)$, $(\\bar{X}_k)$ is the mean of group $(k)$, and$(\\bar{X})$ is the overall mean.\n",
        "\n",
        "  → Calculate the overall mean of all observations.\n",
        "\n",
        "  →  For each group, calculate the squared difference between the group mean and the overall mean, weighted by the number of observations in each group:\n",
        "\n",
        "→ ANOVA uses an F-test to compare the between-group variance to the within-group variance. If the between-group variance is significantly larger than expected by chance, then there are actual differences in the group means.\n",
        "\n",
        " 2. **Within-Group Variance (WGV)**:\n",
        "- **Definition**: -This component captures the variability within each group. It measures how much the individual observations in each group differ from their respective group mean:(Quantifies the spread of the observations within each group )\n",
        "- **Calculation**:\n",
        "\n",
        "$\n",
        "[\n",
        "\\text{Within-Group Sum of Squares (SSW)} = \\sum (X_{ij} - \\bar{X}_k)^2\n",
        "]$\n",
        "\n",
        "where $(X_{ij})$ are the observations in group$ (k)$, and $(\\bar{X}_k)$ is the mean of group $(k)$.\n",
        "  - For each group, calculate the squared differences between individual observations and their group mean, then sum these squared differences across all groups:\n",
        "\n",
        " 3. **Total Variance:**\n",
        "The total variance is the overall variability in the data. It is calculated as the sum of squared deviations from the overall mean:\n",
        "\n",
        "$[\n",
        "\\text{Total Sum of Squares (SST)} = \\sum (X_{ij} - \\bar{X})^2\n",
        "]$\n",
        "\n",
        "where $(X_{ij})$ represents each individual observation, and $(\\bar{X}$ is the overall mean of all observations.\n",
        "\n",
        "4.  **Partitioning the Variance:**\n",
        "The total variance can be expressed as:\n",
        "$\n",
        "[\n",
        "\\text{Total Variance} = \\text{Between-Group Variance} + \\text{Within-Group Variance}\n",
        "]$\n",
        "\n",
        "Mathematically, this is represented as:\n",
        "$\n",
        "[\n",
        "\\text{SST} = \\text{SSB} + \\text{SSW}\n",
        "]$\n",
        "\n",
        "**Contribution to the F-statistic:**\n",
        "The F-statistic is calculated by comparing the mean square values derived from the two components of variance:\n",
        "$\n",
        "[\n",
        "F = \\frac{\\text{Mean Square Between (MSB)}}{\\text{Mean Square Within (MSW)}}\n",
        "]$\n",
        "\n",
        "- **Mean Square Between (MSB)**:\n",
        "  - This is obtained by dividing the between-group variance (BGV) by its degrees of freedom (\\(k - 1\\)):\n",
        "  \n",
        "  $[\n",
        "  \\text{MSB} = \\frac{\\text{BGV}}{k - 1}\n",
        "  ]$\n",
        "\n",
        "- **Mean Square Within (MSW)**:\n",
        "  - This is obtained by dividing the within-group variance (WGV) by its degrees of freedom (\\(N - k\\)), where \\(N\\) is the total number of observations:\n",
        "  \n",
        "  $[\n",
        "  \\text{MSW} = \\frac{\\text{WGV}}{N - k}\n",
        "  ]$\n",
        "\n",
        "→ F-statistic: Calculated by taking the ratio of the Between Mean Sum of Squares to the Error Mean Sum of Squares\n",
        "\n",
        "→ The F-statistic is used to evaluate whether the variance among the groups is greater than the variance within a group. A large F-value indicates that the between-groups variance is larger than the within-group variance, which suggests that not all group means are equal.\n",
        "\n",
        "→ The F-statistic is a unitless statistic, so it's difficult to interpret on its own. To interpret the F-statistic, you can place it in a larger context using an F-distribution, which provides probabilities\n",
        "\n",
        "**Summary:**\n",
        "By comparing the mean square between groups to the mean square within groups, the F-statistic assesses whether the variability due to the differences among group means is greater than the variability within the groups themselves. A high F-value suggests that the group means are significantly different, indicating that the independent variable has a statistically significant effect on the dependent variable. This partitioning of variance is foundational in determining the significance of results in ANOVA.\n",
        "\n",
        "→ The F-statistic provides a measure of the ratio of variance due to the treatment effects (between-group) to the variance due to random error (within-group). A higher F value indicates that the between-group variance is significantly greater than the within-group variance, suggesting that the group means are not all equal. This forms the basis for hypothesis testing in ANOVA. If the F-statistic exceeds a critical value from the F-distribution, we reject the null hypothesis that all group means are equal.\n",
        "\n",
        "→ In summary, the partitioning of variance in ANOVA is crucial for calculating the F-statistic and helps determine whether the observed differences between groups are statistically significant."
      ],
      "metadata": {
        "id": "OGdemUROWPgS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Compare the classical (frequentist) approach to ANOVA with the Bayesian approach. What are the key differences in terms of how they handle uncertainty, parameter estimation, and hypothesis testing?\n",
        "\n",
        "### Ans\n",
        "\n",
        "→ The main difference between the classical (frequentist) and Bayesian approaches to ANOVA is how they handle uncertainty, parameter estimation, and hypothesis testing:\n",
        "\n",
        "1. **handle Uncertainty**\n",
        "\n",
        "Frequentist Approach:\n",
        "\n",
        "→ Uncertainty is expressed through confidence intervals and p-values.\n",
        "It relies on the idea of long-run frequencies: probabilities are associated with the data and the sampling process, not with parameters or hypotheses. For example, a 95% confidence interval indicates that if the experiment were repeated many times, 95% of the calculated intervals would contain the true parameter value.\n",
        "\n",
        "→ Uncertainty is expressed in terms of long-run frequencies. The focus is on the probability of observing data given a specific hypothesis (e.g., null hypothesis). Confidence intervals are used to estimate uncertainty around parameters.\n",
        "\n",
        "\n",
        "Bayesian Approach:\n",
        "\n",
        "→ Uncertainty is modeled using probability distributions.\n",
        "Bayesian statistics combines prior beliefs (prior distributions) with the observed data (likelihood) to produce a posterior distribution, which provides a direct and intuitive measure of uncertainty about parameter estimates.\n",
        "\n",
        "→ Uncertainty is expressed through probability distributions. It allows for the incorporation of prior beliefs about parameters, leading to a more flexible interpretation of uncertainty. The results are interpreted in terms of the probability of parameters given the data.\n",
        "\n",
        "Example\n",
        "\n",
        "\n",
        "→ Frequentist Approach: Suppose we conduct a one-way ANOVA to compare the means of three groups (A, B, and C) based on some measurement. After performing the ANOVA, we calculate a p-value of 0.03.\n",
        "\n",
        "→ Interpretation: This p-value indicates that if the null hypothesis (that all group means are equal) were true, there’s a 3% chance of observing the data or something more extreme. We might reject the null hypothesis if we use a significance level of 0.05, concluding that there is a statistically significant difference among the group means.\n",
        "\n",
        "\n",
        "→ Bayesian Approach: Using the same data, we might start with a prior belief about the means of the groups (e.g., we might believe the means are normally distributed around some value). After observing the data, we update our beliefs using Bayes’ theorem.\n",
        "\n",
        "→ Interpretation: Instead of a p-value, we might find that the posterior probability that the mean of group A is greater than that of group B is 0.92. This means there’s a 92% probability that the mean of group A exceeds that of group B, which provides a direct probabilistic interpretation of the results.\n",
        "\n",
        "*Summary:-*Frequentists assume probabilities are fixed and objective, and rely on long-term frequencies. Bayesians embrace subjectivity and the idea that probabilities change based on new information.\n",
        "\n",
        "2. **Parameter Estimation**\n",
        "\n",
        "\n",
        "Frequentist Approach:\n",
        "\n",
        "→ Parameters are estimated using point estimates, such as the sample mean, and the uncertainty is typically expressed via standard errors and confidence intervals.\n",
        "\n",
        "→ Confidence intervals give a range of plausible values but do not provide a probability statement about where the true parameter lies.\n",
        "\n",
        "→ Parameters (e.g., group means) are estimated using point estimates (e.g., sample means). The method relies on methods like maximum likelihood estimation (MLE) or least squares. It does not incorporate prior distributions.\n",
        "\n",
        "→ -Example: After performing ANOVA, we might report the sample means of groups A, B, and C:\n",
        "Group A mean = 5.0\n",
        "Group B mean = 6.5\n",
        "Group C mean = 5.8\n",
        "The estimates are point estimates (just single numbers), and we might construct a 95% confidence interval around each mean. However, these intervals can be misinterpreted; for instance, they do not tell us the probability that the true mean lies within that interval.\n",
        "\n",
        "\n",
        "Bayesian Approach:\n",
        "\n",
        "→ Parameters are treated as random variables with probability distributions.\n",
        "The posterior distribution provides a complete picture of parameter uncertainty, allowing for direct probabilistic statements about parameters (e.g., \"There is a 70% probability that the true mean is within this interval\").\n",
        "\n",
        "→ Parameters are treated as random variables with prior distributions. Posterior distributions are obtained through Bayes’ theorem, combining prior information with the likelihood from the observed data. This results in full distributions for parameter estimates, providing more comprehensive insights into uncertainty.\n",
        "\n",
        "→Example: In a Bayesian framework, we calculate posterior distributions for each group mean. Instead of a single estimate, we might find:\n",
        "Group A: Posterior mean = 5.0, 95% credible interval = [4.5, 5.5]\n",
        "Group B: Posterior mean = 6.5, 95% credible interval = [6.0, 7.0]\n",
        "Group C: Posterior mean = 5.8, 95% credible interval = [5.3, 6.3]\n",
        "Here, the credible interval indicates that there’s a 95% probability that the true mean lies within that range, which is a more intuitive interpretation than the frequentist confidence interval.\n",
        "\n",
        "*summary:-*Frequentists use point estimates of unknown parameters to predict new data points. Bayesians use a full posterior distribution over possible parameter values to take into account uncertainty in the estimate.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "3. **Hypothesis Testing**\n",
        "\n",
        "Frequentist Approach:\n",
        "\n",
        "→ Hypothesis testing is done using F-tests in ANOVA, focusing on the null hypothesis (e.g., that all group means are equal).\n",
        "\n",
        "→ Decisions are made based on p-values: if the p-value is below a specified significance level (e.g., 0.05), the null hypothesis is rejected. This results in a binary decision (reject or fail to reject), which some critics argue oversimplifies the evidence.\n",
        "\n",
        "→in other word Hypothesis testing involves null and alternative hypotheses. The focus is on p-values, which indicate the probability of observing data at least as extreme as the observed data under the null hypothesis. A significance level (e.g., α = 0.05) is used to decide whether to reject the null hypothesis.\n",
        "\n",
        "→ Example: We set up our null hypothesis (H0: μA = μB = μC) and alternative hypothesis (H1: at least one mean is different). After computing the ANOVA, we obtain a p-value of 0.03.\n",
        "\n",
        "Decision: We reject the null hypothesis because the p-value is less than 0.05. However, we do not get information on how much better one hypothesis fits the data compared to another.\n",
        "\n",
        "\n",
        "Bayesian Approach:\n",
        "\n",
        "→ Hypothesis testing involves calculating the posterior probabilities of hypotheses or using Bayes factors to compare the relative evidence for different hypotheses.\n",
        "\n",
        "→ This allows for a more nuanced interpretation of the evidence, enabling researchers to quantify how much more likely one hypothesis is compared to another, rather than just making a binary decision.\n",
        "\n",
        "→ Hypothesis testing involves comparing the posterior probabilities of competing hypotheses. Instead of p-values, Bayesian methods provide the probability of a hypothesis given the data. This allows for direct interpretation, such as calculating the probability that one group mean is greater than another.\n",
        "\n",
        "→ Example: Instead of setting up null and alternative hypotheses, we can calculate the posterior probabilities of different hypotheses. For example:\n",
        "H0: All means are equal (μA = μB = μC).\n",
        "H1: At least one mean is different.\n",
        "\n",
        "After analysis, we might find P(H0 | data) = 0.1 and P(H1 | data) = 0.9. This indicates that there’s a 90% probability that at least one group mean differs from the others, providing a clear decision-making framework based on probability.\n",
        "\n",
        "*Summary:-*Frequentists assign probabilities to data, not to hypotheses. Bayesians assign probabilities to hypotheses and update them as more data becomes available.\n",
        "\n",
        "**Summary:-**\n",
        "\n",
        "| Aspect                    | Frequentist Approach                                | Bayesian Approach                                      |\n",
        "|---------------------------|---------------------------------------------------|-------------------------------------------------------|\n",
        "| **Handling of Uncertainty** | Long-run frequencies, p-values                    | Probability distributions, posterior probabilities     |\n",
        "| **Parameter Estimation**   | Point estimates, confidence intervals              | Full posterior distributions, credible intervals      |\n",
        "| **Hypothesis Testing**     | Null vs. alternative hypothesis, p-values         | Direct probability of hypotheses given the data       |\n",
        "| **Modeling Flexibility**   | Fixed assumptions, standard models                 | Flexible models, hierarchical structures               |\n",
        "| **Computational Methods**  | Analytical methods, standard software              | MCMC and other simulation methods, computationally intensive |\n",
        "\n",
        "\n",
        "→ Frequentist ANOVA: Relies on observed data and frequentist inference, testing hypotheses with a fixed threshold (e.g., p-value < 0.05).\n",
        "\n",
        "→ Bayesian ANOVA: Incorporates prior beliefs and provides a probability distribution of the parameters, offering probabilistic conclusions about group differences.\n",
        "\n",
        "\n",
        "→ The Bayesian approach can account for uncertainty in parameter estimation differently by updating beliefs with new data, while frequentist methods use fixed sample data.\n",
        "\n",
        "→ In summary, the frequentist approach focuses on long-term frequency properties and emphasizes procedure performance through confidence intervals and p-values, whereas the Bayesian approach incorporates prior beliefs and yields a probabilistic framework for understanding uncertainty and making inferences. Each method has its advantages and is chosen based on the specific context and goals of the analysis.\n",
        "\n"
      ],
      "metadata": {
        "id": "mR-iPmShNZzG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Question: You have two sets of data representing the incomes of two different professions. Profession A: [48, 52, 55, 60, 62'], Profession B: [45, 50, 55, 52, 47] Perform an F-test to determine if the variances of the two professions'incomes are equal. What are your conclusions based on the F-test?Task: Use Python to calculate the F-statistic and p-value for the given data.Objective: Gain experience in performing F-tests and interpreting the results in terms of variance comparison.\n",
        "\n",
        "### Ans\n"
      ],
      "metadata": {
        "id": "ZeKaGGEYXV-O"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "goh3Qr34fKGE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "567ba96d-597d-489d-c42e-42568f4d92b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F-statistic: 2.089171974522293\n",
            "Degrees of Freedom (A): 5\n",
            "Degrees of Freedom (B): 5\n",
            "p-value: 0.21897256042002633\n",
            "Fail to reject the null hypothesis: Variances are equal.\n"
          ]
        }
      ],
      "source": [
        "#Given data\n",
        "profession_A=[48,52,55,60,62]\n",
        "profession_B=[45,50,55,52,47]\n",
        "\n",
        "#perform f-test\n",
        "\n",
        "import numpy as np\n",
        "from scipy.stats import f\n",
        "\n",
        "# Calculate the variances\n",
        "var_A = np.var(profession_A, ddof=1)\n",
        "var_B = np.var(profession_B, ddof=1)\n",
        "\n",
        "# Calculate the F-statistic\n",
        "F_statistic = var_A / var_B\n",
        "\n",
        "# Calculate the degrees of freedom\n",
        "df_A = len(profession_A)\n",
        "df_B = len(profession_B)\n",
        "\n",
        "# Calculate the p-value\n",
        "p_value = 1 - f.cdf(F_statistic, df_A, df_B)\n",
        "\n",
        "# Print the results\n",
        "print(\"F-statistic:\", F_statistic)\n",
        "print(\"Degrees of Freedom (A):\", df_A)\n",
        "print(\"Degrees of Freedom (B):\", df_B)\n",
        "print(\"p-value:\", p_value)\n",
        "\n",
        "#Conclusion\n",
        "if p_value < 0.05:\n",
        "    print(\"Reject the null hypothesis: Variances are not equal.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis: Variances are equal.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusions:-**\n",
        "F-statistic Interpretation: In this example, if the F-statistic is around 2.08, it suggests that the variance of incomes in profession A is somewhat larger than in profession B but not extremely so.\n",
        "\n",
        "p-value Interpretation: With a p-value of approximately 0.21, which is greater than 0.05, we do not reject the null hypothesis. This means there is not enough evidence to conclude that the variances of incomes between the two professions are significantly different."
      ],
      "metadata": {
        "id": "gWtzGNLrbC87"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. Question: Conduct a one-way ANOVA to test whether there are any statistically significant differences in average heights between three different regions with the following data1 Region A: [160, 162, 165, 158, 164'], Region B: [172, 175, 170, 168, 174'],Region C: [180, 182, 179, 185, 183']Task: Write Python code to perform the one-way ANOVA and interpret the results. Objective: Learn how to perform one-way ANOVA using Python and interpret F-statistic and p-value.\n",
        "\n",
        "### Ans"
      ],
      "metadata": {
        "id": "RcHjgg1pbv_C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Given data\n",
        "Region_A=[160,162,165,158,164]\n",
        "Region_B=[172,175,170,168,174]\n",
        "Region_C=[180,182,179,185,183]\n",
        "\n",
        "#Perform one-way ANOVA test\n",
        "\n",
        "from scipy.stats import f_oneway\n",
        "\n",
        "# Perform one-way ANOVA\n",
        "f_statistic, p_value = f_oneway(Region_A, Region_B, Region_C)\n",
        "\n",
        "#print the result\n",
        "print(\"F-statistic:\", f_statistic)\n",
        "print(\"p-value:\", p_value)\n",
        "\n",
        "#conclusion\n",
        "if p_value < 0.05:\n",
        "    print(\"Reject the null hypothesis: There are statistically significant differences in average heights between the regions.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis: There are no statistically significant differences in average heights between the regions.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aPuYcHwpbsZT",
        "outputId": "2b54668e-f7e7-46ed-d706-4a19fc30b220"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F-statistic: 67.87330316742101\n",
            "p-value: 2.870664187937026e-07\n",
            "Reject the null hypothesis: There are statistically significant differences in average heights between the regions.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusion:-**Since the p-value is much less than , we conclude that there are statistically significant differences in average heights between at least two of the regions (Region A, Region B, and Region C). This suggests that the height distributions are not the same across the three regions, indicating a real effect of region on height."
      ],
      "metadata": {
        "id": "E7Ec5rRmEgxI"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xDSJafGTdwOZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}