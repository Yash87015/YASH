{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Name:-Yash solanki\n",
        "\n",
        "Reg email id:-yash87015@gmail.com\n",
        "\n",
        "Course name:-Data analytic\n",
        "\n",
        "Assignment name:- Feature Engineering assignment\n",
        "\n",
        "Submission date:-7/12/2024\n",
        "\n",
        "Git link:-https://github.com/Yash87015/YASH (in git hub use assignment folder )"
      ],
      "metadata": {
        "id": "Z9QWdBWJw2qh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q1.What is a parameter?\n",
        "### Ans\n",
        "→  A parameter typically refers to a specific setting or value that can be adjusted within an algorithm, model, or data transformation process. These settings or values influence how features are processed, selected, or transformed in order to improve the performance of a machine learning model.\n",
        "\n",
        "→  \"parameter\" within the context of feature engineering refers to a variable that is directly learned by the model during training, typically based on the data provided, and is used to create new features or transform existing ones, essentially defining the model's internal representation of the data used for prediction.\n",
        "\n",
        "→ Key points about parameters in feature engineering:\n",
        "\n",
        " **Learned from data:-**Unlike hyperparameters, parameters are automatically adjusted by the model during training based on the data it sees.\n",
        "\n",
        "**Impact on feature creation:-**In feature engineering, parameters can be used within transformations like scaling, normalization, or polynomial feature creation to adjust how features are manipulated.\n",
        "\n",
        "**Internal to the model:-**Parameters are considered internal variables within the model and cannot be directly set by the user."
      ],
      "metadata": {
        "id": "m29_8kyBxCOX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q2. What is correlation? What does negative correlation mean?\n",
        "### Ans\n",
        "\n",
        "→ **Correlation:-** Correlation  describes the relationship between two variables.\n",
        "\n",
        "→ negative correlation means that the variables move in opposite directions:\n",
        "\n",
        "**Positive correlation:-**When two variables move in the same direction, meaning that as one variable increases, so does the other.In other word a relationship between two variables where they both increase or decrease together. It is represented by a correlation coefficient ranging from 0 to +1, indicating a strong positive relationship between the variables.\n",
        "\n",
        "**Negative correlation:-**When two variables move in opposite directions, meaning that as one variable increases, the other decreases.In other words, when variable A increases, variable B decreases. A negative correlation is also known as an inverse correlation. Two variables can have varying strengths of negative correlation.\n",
        "\n",
        "→ Correlation coefficients are used to indicate the strength of the relationship between two variables. A coefficient of +1.0 indicates a perfect positive correlation, while a coefficient of -1.0 indicates a perfect negative correlation.\n"
      ],
      "metadata": {
        "id": "2fpJ4N83062D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q3 Define Machine Learning. What are the main components in Machine Learning?\n",
        "\n",
        "### Ans\n",
        "\n",
        "→ Machine Learning:-Machine Learning is a subset of artificial intelligence that allows computers to learn from data without explicit programming, identifying patterns and making predictions based on the information provided, essentially improving their performance over time through experience.\n",
        "\n",
        "→The main components of machine learning include: data sets, algorithms, models, feature extraction, and evaluation metrics used to train and assess the learning process\n",
        "\n",
        "→ Data Sets:-The raw information used to train the machine learning model, which can be labeled (supervised learning) or unlabeled (unsupervised learning).\n",
        "\n",
        "→ Algorithms:-Mathematical equations or procedures that process the data to identify patterns and make predictions.\n",
        "\n",
        "→ Models:-The representation of the learned patterns from the data, which can be used to make predictions on new data.\n",
        "\n",
        "→ Feature Extraction:-The process of selecting relevant features from the data to improve the model's accuracy.\n",
        "\n",
        "→ Evaluation Metrics:-Measures used to assess the performance of a machine learning model, like accuracy, precision, recall, and F1-score.\n",
        "\n",
        "**Types of Machine Learning:-**\n",
        "\n",
        "→ Supervised Learning:-The model is trained on labeled data where the desired output is already known, allowing it to learn the relationship between input and output.\n",
        "\n",
        "→ Unsupervised Learning:-The model analyzes unlabeled data to identify patterns and clusters without explicit guidance.\n",
        "\n",
        "→ Reinforcement Learning:-The model learns through trial and error, receiving feedback (rewards or penalties) for its actions in an environment."
      ],
      "metadata": {
        "id": "-A1LZJIG3dXD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q4. How does loss value help in determining whether the model is good or not?\n",
        "### Ans\n",
        "\n",
        "→ fist of all lets detrmind what is loss value ?\n",
        "→ Ans:-Loss value is a numerical metric that measures the difference between a model's predictions and the actual labels.\n",
        "\n",
        "→ Loss is a numerical metric that describes how wrong a model's predictions are. Loss measures the distance between the model's predictions and the actual labels. The goal of training a model is to minimize the loss, reducing it to its lowest possible value.It represents the difference between the model’s predictions and the actual outcomes (ground truth).\n",
        "\n",
        "**Indicates Model Performance:**\n",
        "\n",
        "→ Lower Loss = Better Model: A lower loss value means that the model's predictions are closer to the true values, indicating better performance. The model is effectively capturing the underlying patterns in the data.\n",
        "\n",
        "→ Higher Loss = Poorer Model: A higher loss value indicates that the model's predictions are far from the actual outcomes, signaling that the model is not learning well and is performing poorly."
      ],
      "metadata": {
        "id": "cul-Lhr_7ZfW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q5.What are continuous and categorical variables?\n",
        "\n",
        "### Ans\n",
        "→ Continuous Variables:- Continuous variables are quantitative variables that can take on an infinite number of values within a given range. These variables can represent measurements and can be divided into smaller, more precise intervals.\n",
        "\n",
        "→ Example:- Examples include weight, height, temperature,\n",
        "\n",
        "→ Categorical Variables:- Categorical variables are qualitative variables that represent categories or groups. These variables can take on a limited, fixed number of values or categories, which may or may not have any meaningful order.\n",
        "Are descriptive and not numerical, and consist of discrete values that fall into distinct categories or groups.\n",
        "\n",
        "→ Example:- include hair color, gum flavor, dog breed, gender,\n",
        "\n",
        "Categorical variables can be further divided into two types:\n",
        "\n",
        "Nominal variables:-Not ordered, and the order doesn't matter. For example, eye color is nominal because there is no higher or lower eye color.\n",
        "\n",
        "Ordinal variables:-Have two or more categories that can be ordered or ranked. For example, someone might answer a question about their policy preferences as \"Not very much\", \"They are OK\", or \"Yes, a lot\".\n"
      ],
      "metadata": {
        "id": "jUIn8yYz-RFW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q6.How do we handle categorical variables in Machine Learning? What are the common techniques?\n",
        "\n",
        "### Ans\n",
        "\n",
        "Handling **categorical variables** in machine learning is an essential step because many machine learning algorithms require numerical input data. Since categorical variables are non-numeric, they need to be transformed into a numerical format that models can understand. There are several common techniques for encoding categorical variables:\n",
        "\n",
        " 1. Label Encoding:- Label encoding converts each category in a categorical variable to a unique integer. For example, if you have a \"Color\" column with values like \"Red,\" \"Blue,\" and \"Green,\" label encoding will assign each color a numeric value like 0, 1, and 2, respectively.\n",
        "   \n",
        "   - When to use: Label encoding is useful when the categorical variable has an inherent order (ordinal data), such as education level (e.g., High School = 0, Bachelor's = 1, Master's = 2). It’s also efficient for variables with many categories when the model can handle numerical relationships.\n",
        "   \n",
        "\n",
        "\n",
        "\n",
        " 2. One-Hot Encoding:- One-hot encoding creates a binary (0 or 1) column for each possible category in the original feature. This means each category is represented as a separate column, and a \"1\" is placed in the corresponding column for that observation.\n",
        "   \n",
        "   - When to use: One-hot encoding is ideal for nominal categorical variables (variables with no inherent order), such as \"Country\" (USA, India, Canada). It prevents any assumption of ordinal relationships and ensures that each category is treated independently.\n",
        "\n",
        "\n",
        " 3. Binary Encoding:- Binary encoding is a more efficient version of one-hot encoding. First, each category is assigned a unique integer, then the integer is converted into binary code, and each bit of the binary code is used as a separate feature. This can be useful when the categorical variable has many levels but we want to reduce dimensionality.\n",
        "\n",
        "   - When to use: Binary encoding is useful for categorical variables with a large number of categories. It helps reduce the dimensionality compared to one-hot encoding while still encoding the information effectively.\n",
        "\n",
        "   - **Example**:\n",
        "     Assume a \"Color\" variable with three categories: Red, Blue, Green.\n",
        "     - Red → 0 → 00\n",
        "     - Blue → 1 → 01\n",
        "     - Green → 2 → 10\n",
        "\n",
        "\n",
        " 4. Frequency (or Count) Encoding:- Frequency encoding involves replacing each category with the frequency or count of its occurrence in the dataset. It maps each category to the number of times it appears in the dataset.\n",
        "   \n",
        "   - When to use: Frequency encoding is useful when categorical variables have many levels, and you want to capture information about the distribution of categories. It can work well with nominal data but should be used carefully with ordinal data.\n",
        "   \n",
        "   - **Example**:\n",
        "     If the \"Color\" variable has the following frequency distribution:\n",
        "     - Red: 5 occurrences\n",
        "     - Blue: 3 occurrences\n",
        "     - Green: 2 occurrences\n",
        "   \n",
        "  \n",
        " 5. Target (Mean) Encoding:- Target encoding replaces each category with the mean of the target variable (dependent variable) for that category. For example, if you are predicting house prices, the average price for houses in each neighborhood is used as the encoded value for that neighborhood.\n",
        "\n",
        "   - **When to use**: Target encoding is effective when there’s a strong relationship between the categorical feature and the target variable, especially in regression tasks or when there’s a strong correlation between category and target.\n",
        "\n",
        "   - **Example**:\n",
        "\n",
        "     | Color       | Target Encoding (Mean Price) |\n",
        "     \n",
        "     | Red         | 200,000   |\n",
        "\n",
        "     | Blue        | 250,000  |\n",
        "\n",
        "     | Green        | 180,000  |\n",
        "\n",
        "\n",
        "\n",
        "\n",
        " Summary of Common Techniques:\n",
        "\n",
        "| Technique             | Description                                                                 | When to Use                                | Pros                                       | Cons                                        |\n",
        "|-----------------------|-----------------------------------------------------------------------------|--------------------------------------------|--------------------------------------------|---------------------------------------------|\n",
        "| **Label Encoding**     | Convert categories to integers.                                              | Ordinal data, low cardinality categories.  | Simple, efficient.                         | Assumes ordinal relationship, which may not exist. |\n",
        "| **One-Hot Encoding**   | Create binary columns for each category.                                     | Nominal data with few categories.          | No assumptions about order.                | High dimensionality with many categories.  |\n",
        "| **Binary Encoding**    | Convert categories to binary and split bits into columns.                    | Large cardinality, when one-hot encoding is inefficient. | Reduces dimensionality compared to one-hot. | May cause information loss for some categories. |\n",
        "| **Frequency Encoding** | Replace categories with their frequency in the dataset.                      | Large cardinality, when order is irrelevant. | Reduces dimensionality.                    | May lose information, problematic with uniform distribution. |\n",
        "| **Target Encoding**    | Replace categories with the mean of the target for that category.            | Strong relationship with the target variable. | Can capture category-target relationship well. | Risk of overfitting, especially with small datasets. |\n",
        "\n",
        "\n",
        "→ Conclusion:-The choice of technique for encoding categorical variables depends on the nature of the data (whether it’s nominal or ordinal, and the cardinality of the variable), the type of machine learning model you're using, and the computational resources available. Each technique has its trade-offs in terms of complexity, memory usage, and potential for overfitting."
      ],
      "metadata": {
        "id": "Hx8K5f58COEi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q7.What do you mean by training and testing a dataset?\n",
        "### Ans\n",
        "\n",
        "→ Training and Testing a dataset:- Refers to the process of splitting a dataset into two parts: a \"training set\" which is used to teach a model to recognize patterns and make predictions, and a separate \"testing set\" which is used to evaluate how well the trained model performs on new, unseen data, essentially checking its accuracy and generalizability.\n",
        "\n",
        "→ Training data:-This is the larger portion of the dataset used to train the model, allowing it to learn the underlying relationships and patterns within the data.\n",
        "\n",
        "→ Testing data:-This is a separate portion of the dataset held back from the training process, used to assess the model's performance on new data it hasn't seen before.\n",
        "\n",
        "→ Why it's important:-By using separate training and testing sets, you can avoid overfitting, where a model learns the training data too well and performs poorly on new data.\n",
        "\n",
        "→Example:Imagine you want to train a model to classify images of cats and dogs.\n",
        "\n",
        "Training set: A large collection of labeled images where the model learns to identify features that distinguish cats from dogs.\n",
        "\n",
        "Testing set: A separate set of labeled cat and dog images that the model has never seen before, used to evaluate how accurately it can classify new images."
      ],
      "metadata": {
        "id": "wI3_S2J-GX0m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q8.What is sklearn.preprocessing?\n",
        "### Ans\n",
        "\n",
        "→ The sklearn. preprocessing package(sklearn.preprocessing is a module in the scikit-learn library ) provides several common utility functions and transformer classes to change raw feature vectors into a representation that is more suitable for the downstream estimators. In general, learning algorithms benefit from standardization of the data set.\n",
        "\n",
        "→here some of the example like Scaling:\n",
        "It offers methods to scale features to a specific range or distribution, such as:\n",
        "\n",
        "StandardScaler: Standardizes features by removing the mean and scaling to unit variance.\n",
        "\n",
        "MinMaxScaler: Scales features to a specified range (e.g.,).\n",
        "\n",
        "RobustScaler: Scales features using statistics that are robust to outliers, like the median and interquartile range.\n",
        "\n",
        "→Many more example are also ther like OHE and other as well"
      ],
      "metadata": {
        "id": "9WvyVzZ8I_l2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q9. What is a Test set?\n",
        "### Ans\n",
        "\n",
        "→ A \"test set\" is a portion of data that is held aside from the training data in machine learning, used to evaluate the performance of a model on completely unseen data, allowing for an accurate assessment of how well the model generalizes to new situations;Once the model is trained, it is applied to the test set, and its performance is assessed using relevant evaluation metrics like accuracy, precision, recall, etc"
      ],
      "metadata": {
        "id": "RT5DukLhNTwS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q10. How do we split data for model fitting (training and testing) in Python?How do you approach a Machine Learning problem?\n",
        "\n",
        "### Ans\n",
        "\n",
        "→ To split data for model fitting in Python, you typically use the train_test_split function from the scikit-learn library, which allows you to divide your dataset into a training set (used to train the model) and a testing set (used to evaluate the model's performance on unseen data); you specify the proportion of data you want to allocate to the test set using the test_size parameter.\n",
        "\n",
        "→ below example is there"
      ],
      "metadata": {
        "id": "6SvfZTA0N7Es"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#example\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "# Example dataset (can be replaced with your actual dataset)\n",
        "data = {'feature1': [1, 2, 3, 4, 5],\n",
        "        'feature2': [5, 4, 3, 2, 1],\n",
        "        'target': [0, 1, 0, 1, 0]}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Define your features (X) and target (y)\n",
        "X = df.drop('target', axis=1)  # Features (independent variables)\n",
        "y = df['target']  # Target variable (dependent variable)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "# test_size=0.2 means 20% of the data will be used for testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Check the split data\n",
        "print(\"Training Features:\")\n",
        "print(X_train)\n",
        "print(\"Testing Features:\")\n",
        "print(X_test)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "Qz3l9MdK3Lmv",
        "outputId": "1ff52c70-b4e7-4dd1-c487-c278470a814d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Features:\n",
            "   feature1  feature2\n",
            "4         5         1\n",
            "2         3         3\n",
            "0         1         5\n",
            "3         4         2\n",
            "Testing Features:\n",
            "   feature1  feature2\n",
            "1         2         4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**How to approach a Machine Learning problem:-**\n",
        "\n",
        "1. Define the problem:\n",
        "Clearly understand the task you want to solve, including the target variable you want to predict and the available features.\n",
        "2. Data Acquisition and Preprocessing:\n",
        "Gather data: Collect a relevant dataset for your problem.\n",
        "Clean data: Handle missing values, outliers, and inconsistencies.\n",
        "Feature engineering: Create new features or transform existing ones to improve model performance.\n",
        "3. Data Splitting:\n",
        "Train-Test Split: Divide your data into training and testing sets.\n",
        "Consider validation split: For fine-tuning hyperparameters, sometimes a separate validation set is used.\n",
        "4. Choose a model:\n",
        "Select a suitable machine learning algorithm based on the problem type (regression, classification, etc.).\n",
        "5. Model Training:\n",
        "Fit the model: Train the model on the training data.\n",
        "Hyperparameter tuning: Optimize model parameters to improve performance.\n",
        "6. Model Evaluation:\n",
        "Make predictions: Use the trained model to predict on the testing data.\n",
        "Evaluate metrics: Calculate relevant performance metrics (e.g., accuracy, precision, recall) based on the problem.\n",
        "7. Deployment:\n",
        "Productionalize: Integrate the trained model into your application to make predictions on new data"
      ],
      "metadata": {
        "id": "GNW4GPHJP6e6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q11. Why do we have to perform EDA before fitting a model to the data?\n",
        "\n",
        "### Ans\n",
        "\n",
        "→ Exploratory Data Analysis (EDA) is important to perform before fitting a model to data because it helps ensure the data is accurate and ready for analysis:\n",
        "\n",
        "→ By performing EDA in the early stages after data collection, data analysts can more effectively assess data quality and fit the appropriate model without being limited by preconceived notions. This can maximize potential insights into the data structure and variable relationships.\n",
        "\n",
        "1. Identify errors: EDA can help identify and fix errors in the data, such as missing values, incorrect labels, or duplicate data.\n",
        "\n",
        "2. Understand data structure: EDA can help break down complex data sets into more manageable chunks.\n",
        "\n",
        "3. Uncover patterns: EDA can help identify patterns and relationships in the data, such as correlations between variables or trends over time.\n",
        "\n",
        "4. Inform feature selection: EDA can help inform the selection of relevant input features for the model.\n",
        "\n",
        "5. Improve model results: EDA can help guide machine learning choices and improve the results.\n",
        "\n",
        "6. Raw data can often be skewed, have outliers, or have too many missing values, which can lead to poor model performance"
      ],
      "metadata": {
        "id": "QxAbEaKpQH6a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q12.What is correlation?\n",
        "## Q13. What does negative correlation mean?\n",
        "\n",
        "### Ans This questions is repeatative so please refred Q2 for thier ans"
      ],
      "metadata": {
        "id": "G9VxLK12Ru3f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q14. How can you find correlation between variables in Python?\n",
        "### Ans\n",
        "\n",
        "→ to find correlation between variable is multiple way but we mostly follow 2 way . first usinf pand libray df.corr() and another method is visualization:-in this method we use heat map for varias features and compare to other features as well if their was positive corr or negative correlationship\n",
        "\n",
        "→ lets see in detail as well other method also\n",
        "\n",
        "To find the correlation between variables in Python, you can use several methods, with Pandas providing an easy-to-use and efficient way to compute correlation between variables in a DataFrame.\n",
        "\n",
        "\n",
        "\n",
        "1. **Pearson correlation** (most common, measures linear relationships)\n",
        "2. **Spearman rank correlation** (measures monotonic relationships)\n",
        "3. **Kendall rank correlation** (another measure of monotonic relationships)\n",
        "\n",
        "**Using Pandas to Compute Correlation**\n",
        "\n",
        "You can compute correlations using the **`corr()`** method in Pandas. By default, **`corr()`** computes the **Pearson correlation** between columns in a DataFrame.\n",
        "\n",
        "\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "\n",
        "# Example dataset\n",
        "data = {\n",
        "    'feature1': [1, 2, 3, 4, 5],\n",
        "    'feature2': [5, 4, 3, 2, 1],\n",
        "    'feature3': [1, 3, 5, 7, 9]\n",
        "}\n",
        "\n",
        "# Create a DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Compute Pearson correlation between the columns\n",
        "correlation_matrix = df.corr()\n",
        "\n",
        "# Print the correlation matrix\n",
        "print(correlation_matrix)\n",
        "```\n",
        "\n",
        " **Output** (for Pearson correlation):\n",
        "\n",
        "```\n",
        "           feature1  feature2  feature3\n",
        "feature1        1.0       -1.0       1.0\n",
        "feature2       -1.0        1.0      -1.0\n",
        "feature3        1.0       -1.0       1.0\n",
        "```\n",
        "\n",
        "In the correlation matrix:\n",
        "- **1.0** indicates a perfect positive correlation.\n",
        "- **-1.0** indicates a perfect negative correlation.\n",
        "- **0.0** indicates no correlation.\n",
        "\n",
        " **Other Correlation Methods:**\n",
        "\n",
        "You can compute other types of correlations such as **Spearman** and **Kendall** by passing the `method` argument to the **`corr()`** function.\n",
        "\n",
        "1. **Spearman** (non-parametric, measures monotonic relationships):\n",
        "   ```python\n",
        "   spearman_corr = df.corr(method='spearman')\n",
        "   print(spearman_corr)\n",
        "   ```\n",
        "\n",
        "2. **Kendall** (another non-parametric method):\n",
        "   ```python\n",
        "   kendall_corr = df.corr(method='kendall')\n",
        "   print(kendall_corr)\n",
        "   ```\n",
        "\n",
        "### **Visualizing Correlation**\n",
        "\n",
        "To better understand the correlation between features, you can use a **heatmap** for visualization. This is especially useful for datasets with many variables.\n",
        "\n",
        "#### Example: Visualizing Correlation Matrix with Seaborn\n",
        "\n",
        "```python\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generate the correlation matrix\n",
        "correlation_matrix = df.corr()\n",
        "\n",
        "# Plotting the correlation matrix as a heatmap\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "\n",
        " **Interpreting the Correlation Coefficient:**\n",
        "\n",
        "- +1: Perfect positive correlation. As one variable increases, the other increases in a perfectly linear manner.\n",
        "- 0: No correlation. Changes in one variable do not affect the other.\n",
        "- -1: Perfect negative correlation. As one variable increases, the other decreases in a perfectly linear manner.\n",
        "\n",
        "Generally, a correlation value between **0.3 to 0.7** indicates a moderate positive correlation, while **-0.3 to -0.7** indicates a moderate negative correlation.\n",
        "\n",
        " **Summary:**\n",
        "\n",
        "- Use **`df.corr()`** for Pearson correlation.\n",
        "- Specify the `method` parameter for **Spearman** or **Kendall** correlations.\n",
        "- Visualize correlation with **seaborn.heatmap()**.\n",
        "\n"
      ],
      "metadata": {
        "id": "qKHSCEmVSN6A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q15. What is causation? Explain difference between correlation and causation with an example.\n",
        "\n",
        "### Ans\n",
        "\n",
        "→ Causation is when one event directly causes another, while correlation is when two things happen together but one doesn't cause the other.\n",
        "\n",
        "→ Correlation:- Correlation refers to a statistical relationship between two variables. It means that when one variable changes, the other tends to change in a specific way as well, but this doesn't imply that one causes the other to change.\n",
        "\n",
        "→ example:- There might be a correlation between ice cream sales and the number of people swimming in a pool during summer. However, buying ice cream doesn't cause people to swim, nor does swimming cause people to buy ice cream. Both events are influenced by the warmer weather.\n",
        "\n",
        "→ Causation:- Causation (or causal relationship) means that one event directly causes another. If A causes B, then A is the reason why B occurs. This is a stronger relationship than correlation because it implies a cause-and-effect link.\n",
        "\n",
        "→ Example:- Smoking causes lung cancer. In this case, smoking is the cause (A) and lung cancer is the effect (B). This is a direct cause-and-effect relationship.\n",
        "\n",
        "**case sinario of example:-**\n",
        "\n",
        "→Example-1: There might be a correlation between the number of people who wear sunglasses and the amount of ice cream sold. This correlation likely exists because both are influenced by a third factor, which is hot weather.\n",
        "\n",
        "Why not causation? Wearing sunglasses doesn't cause people to buy ice cream, and buying ice cream doesn't cause people to wear sunglasses. They are both simply influenced by the same factor: sunny and hot weather.\n",
        "\n",
        "→ Example2:- Eating junk food (A) can cause weight gain (B). If you repeatedly consume high-calorie, low-nutrient food, it leads to an increase in body fat, thus weight gain.\n",
        "\n",
        "Why causation? There is a clear cause-and-effect link. Overconsumption of junk food is a direct contributor to weight gain."
      ],
      "metadata": {
        "id": "MA4mBMWvVYkK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q16. What is an Optimizer? What are different types of optimizers? Explain each with an example.\n",
        "\n",
        "### Ans\n",
        "→ An optimizer in machine learning and deep learning is an algorithm or method used to minimize or maximize an objective function (usually the loss function**). The objective of an optimizer is to update the model parameters (such as weights and biases) during training to minimize the error and improve the model’s performance.\n",
        "\n",
        "→ Optimizers play a critical role in training models, particularly in tasks like classification and regression, where the model parameters (weights) are iteratively adjusted to reduce the error (loss). Different optimizers use various strategies to update the parameters, and the choice of optimizer can significantly impact the model's efficiency, convergence speed, and performance.\n",
        "\n",
        "→ Optimization is an important component of the training process, as it involves finding the optimal set of parameters for the model that can minimize the loss or error on the training data. Optimizers are algorithms used to find the optimal set of parameters for a model during the training process.\n",
        "\n",
        " **Types of Optimizers**\n",
        "\n",
        "There are several optimizers commonly used in machine learning and deep learning, with different techniques to update the model’s weights.\n",
        "\n",
        "1. **Gradient Descent (GD)**\n",
        "2. **Stochastic Gradient Descent (SGD)**\n",
        "3. **Mini-Batch Gradient Descent**\n",
        "4. **Momentum**\n",
        "5. **Nesterov Accelerated Gradient (NAG)**\n",
        "6. **Adagrad**\n",
        "7. **RMSprop**\n",
        "8. **Adam**\n",
        "9. **Adadelta**\n",
        "10. **FTRL (Follow The Regularized Leader)**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        " 1. Gradient Descent (GD)\n",
        "\n",
        "- Explanation:-Gradient Descent is the simplest optimization algorithm. It works by computing the gradient of the loss function (i.e., how much the loss changes with respect to the parameters) and updating the model parameters in the opposite direction of the gradient to minimize the loss function.\n",
        "  - It uses the entire dataset to compute the gradient for every update (this is called batch gradient descent).\n",
        "\n",
        "- Update Rule:\n",
        "   $[\n",
        "  \\theta = \\theta - \\alpha \\cdot \\nabla_{\\theta} J(\\theta)\n",
        "  ] $\n",
        "  Where:\n",
        "  - $ ( \\theta ) $ are the parameters (weights),\n",
        "  -  $( \\alpha ) $ is the learning rate,\n",
        "  -  $( \\nabla_{\\theta} J(\\theta) ) $ is the gradient of the loss function.\n",
        "\n",
        "- Example: If the objective is to fit a linear regression model, GD can be used to update the weights and minimize the mean squared error.\n",
        "\n",
        "\n",
        " 2. Stochastic Gradient Descent (SGD)\n",
        "\n",
        "- Explanation:\n",
        "  - Unlike Gradient Descent, which uses the entire dataset to compute gradients, Stochastic Gradient Descent (SGD) updates the model parameters after each data point (sample).\n",
        "  - This leads to a much faster convergence, but with more variance in the updates. It’s generally used when the dataset is large and computing gradients over the entire dataset is computationally expensive.\n",
        "\n",
        "- Update Rule:\n",
        "   $[\n",
        "  \\theta = \\theta - \\alpha \\cdot \\nabla_{\\theta} J(\\theta, x_i, y_i)\n",
        "  ] $\n",
        "  Where:\n",
        "  -  $( x_i, y_i ) $ are individual data points (features and target).\n",
        "\n",
        "- Example: In a classification task, each time you process a single sample, you update the parameters based on the gradient of the loss with respect to that sample.\n",
        "\n",
        "\n",
        "\n",
        " 3. Mini-Batch Gradient Descent\n",
        "\n",
        "- Explanation:-Mini-Batch Gradient Descent is a compromise between Batch Gradient Descentand Stochastic Gradient Descent. It splits the dataset into small batches and computes the gradient for each mini-batch, which reduces the variance compared to SGD while being more computationally efficient than full-batch GD.\n",
        "  \n",
        "- Update Rule:\n",
        "   $[\n",
        "  \\theta = \\theta - \\alpha \\cdot \\frac{1}{m} \\sum_{i=1}^{m} \\nabla_{\\theta} J(\\theta, x_i, y_i)\n",
        "  ] $\n",
        "  Where:\n",
        "  -  $( m ) $ is the batch size.\n",
        "  \n",
        "- Example: If you’re training a neural network, you can use mini-batch gradient descent to update the weights after processing a batch of samples, which helps balance the training time and convergence speed.\n",
        "\n",
        "\n",
        " 4. Momentum\n",
        "\n",
        "- Explanation:-Momentum optimizes the basic gradient descent algorithm by adding a fraction of the previous update to the current update. This helps accelerate gradients vectors in the right directions and dampens oscillations.\n",
        "  \n",
        "- Update Rule:\n",
        "   $[\n",
        "  v_t = \\beta v_{t-1} + (1 - \\beta) \\nabla_{\\theta} J(\\theta)\n",
        "  ] $\n",
        "\n",
        "   $[\n",
        "  \\theta = \\theta - \\alpha v_t\n",
        "  ] $\n",
        "  Where:\n",
        "  -  $( v_t ) $ is the velocity (previous gradients),\n",
        "  -  $( \\beta ) $ is the momentum term (usually set around 0.9).\n",
        "\n",
        "- Example: In training deep networks, momentum helps reduce oscillations and can accelerate convergence in areas where the gradient is small.\n",
        "\n",
        "\n",
        "5. Nesterov Accelerated Gradient (NAG)\n",
        "\n",
        "- Explanation:-Nesterov Accelerated Gradient improves upon momentum by calculating the gradient after moving a small step along the direction of the previous velocity. This anticipates the future gradient, leading to faster convergence.\n",
        "  \n",
        "- Update Rule:\n",
        "   $[\n",
        "  v_t = \\beta v_{t-1} + \\nabla_{\\theta} J(\\theta - \\alpha \\beta v_{t-1})\n",
        "  ] $\n",
        "\n",
        "   $[\n",
        "  \\theta = \\theta - \\alpha v_t\n",
        "  ] $\n",
        "\n",
        "- Example: NAG can be used in deep neural networks for faster convergence, especially in the presence of a lot of local minima.\n",
        "\n",
        "\n",
        "\n",
        " 6. Adagrad\n",
        "\n",
        "- Explanation:-Adagrad adjusts the learning rate for each parameter based on the historical gradients. Parameters that have been updated more frequently receive smaller updates, while less frequently updated parameters get larger updates. This helps handle sparse data efficiently.\n",
        "  \n",
        "- Update Rule:\n",
        "   $[\n",
        "  \\theta = \\theta - \\frac{\\alpha}{\\sqrt{G_t + \\epsilon}} \\cdot \\nabla_{\\theta} J(\\theta)\n",
        "  ] $\n",
        "  Where:\n",
        "  -  $( G_t ) $ is the sum of the squared gradients up to time step ( t ),\n",
        "  -  $( \\epsilon ) $ is a small constant to prevent division by zero.\n",
        "\n",
        "- Example: Adagrad is particularly useful in natural language processing (NLP) where data is sparse and some features are updated more frequently than others.\n",
        "\n",
        "\n",
        " 7. RMSprop\n",
        "\n",
        "- Explanation:-RMSprop is an adaptive learning rate method that overcomes the shortcomings of Adagrad by maintaining a moving average of squared gradients and using it to normalize the gradient.\n",
        "  \n",
        "- Update Rule:\n",
        "\n",
        "  $[\n",
        "  v_t = \\beta v_{t-1} + (1 - \\beta) \\nabla_{\\theta} J(\\theta)^2\n",
        "  ] $\n",
        "\n",
        "  $    [\n",
        "      \\theta = \\theta - \\frac{\\alpha}{\\sqrt{v_t + \\epsilon}} \\cdot \\nabla_{\\theta} J(\\theta)\n",
        "  ] $\n",
        "\n",
        "- Example: RMSprop is commonly used in recurrent neural networks (RNNs) and deep networks, particularly in tasks like speech recognition.\n",
        "\n",
        "\n",
        "\n",
        "8. Adam (Adaptive Moment Estimation)\n",
        "\n",
        "- Explanation: -Adam combines the benefits of Momentum and RMSprop by maintaining a moving average of both the gradients (first moment) and the squared gradients (second moment).\n",
        "  \n",
        "- Update Rule:\n",
        " $ [\n",
        "  m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) \\nabla_{\\theta} J(\\theta)]\n",
        "  $\n",
        "\n",
        "  $[\n",
        "  v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) \\nabla_{\\theta} J(\\theta)^2\n",
        "  ]$\n",
        "\n",
        "  $[\n",
        "  \\hat{m_t} = \\frac{m_t}{1 - \\beta_1^t}, \\quad \\hat{v_t} = \\frac{v_t}{1 - \\beta_2^t}\n",
        "  ]$\n",
        "\n",
        "  $[\n",
        "  \\theta = \\theta - \\frac{\\alpha}{\\sqrt{\\hat{v_t}} + \\epsilon} \\cdot \\hat{m_t}\n",
        "  ]$\n",
        "\n",
        "- Example: Adam is widely used for training deep learning models, including convolutional neural networks (CNNs) and recurrent neural networks (RNNs), because of its efficiency and ease of use.\n",
        "\n",
        "\n",
        "\n",
        " 9. Adadelta\n",
        "\n",
        "- Explanation:-Adadelta** is an extension of Adagrad that attempts to reduce its aggressive, monotonically decreasing learning rates by using a moving average of squared gradients.\n",
        "\n",
        "- Update Rule:\n",
        "  $[\n",
        "  \\theta = \\theta - \\frac{\\alpha}{\\sqrt{E[g^2] + \\epsilon}} \\cdot g\n",
        "  ]$\n",
        "  Where $( E[g^2] )$ is the moving average of squared gradients.\n",
        "\n",
        "\n",
        "\n",
        "10. FTRL (Follow The Regularized Leader)\n",
        "\n",
        "- Explanation:\n",
        "  - FTRL is designed for large-scale optimization problems, particularly in sparse datasets, such as those encountered in online advertising.\n",
        "  \n",
        "- Use Case: It’s used for large-scale machine learning models that need to work with sparse features.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MPVEwHaaYoi7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q17. What is sklearn.linear_model ?\n",
        "\n",
        "### Ans\n",
        "→ sklearn.linear_model is a module in the Scikit-learn library that provides a range of classes and functions for implementing linear models in machine learning. These models are used for both regression and classification tasks where the relationship between the features and the target variable is assumed to be linear.\n",
        "\n",
        "\n",
        "→Some of the key classes in this module include:\n",
        "1. LinearRegression: Implements ordinary least squares linear regression.\n",
        "2. Ridge: Implements linear regression with L2 regularization.\n",
        "3. Lasso: Implements linear regression with L1 regularization.\n",
        "4. ElasticNet: Implements linear regression with a combination of L1 and L2 regularization.\n",
        "5. LogisticRegression: Implements logistic regression for classification tasks.\n",
        "6. SGDRegressor: Implements stochastic gradient descent for linear regression.\n",
        "7. SGDClassifier: Implements stochastic gradient descent for classification."
      ],
      "metadata": {
        "id": "jy6j9LLRdhxy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q18. What does model.fit() do? What arguments must be given?\n",
        "\n",
        "### Ans\n",
        "\n",
        "→ Model fitting is a procedure that takes three steps: First you need a function that takes in a set of parameters and returns a predicted data set. Second you need an 'error function' that provides a number representing the difference between your data and the model's prediction for any given set of model parameters.\n",
        "\n",
        "→ What does model.fit() do?\n",
        "Training the Model: It trains the model on the input training data by adjusting its parameters to minimize the error (loss) with respect to the given labels (or targets).\n",
        "\n",
        "→ Iterative Process: During training, fit() runs through the data multiple times (called epochs) to improve the model’s performance by updating its parameters through backpropagation (in deep learning) or gradient descent (in general machine learning).\n",
        "\n",
        "→ Optimization: The optimizer (e.g., Adam, SGD) is used to minimize the loss function (e.g., mean squared error, cross-entropy) by adjusting the model’s parameters during each iteration.\n",
        "\n",
        "→ Arguments for model.fit():\n",
        "The arguments required for the fit() method depend on the specific library or framework you're using (such as scikit-learn, TensorFlow/Keras, etc.). Here’s a breakdown of common arguments across popular frameworks:\n",
        "\n",
        "1. scikit-learn (model.fit() in scikit-learn):-In scikit-learn, model.fit() is used to train models such as regression or classification models.\n",
        "\n",
        "→ Common Arguments:\n",
        "X_train: The training data (features). It is usually a 2D array (matrix) where each row represents a data point and each column represents a feature.\n",
        "\n",
        "Shape: (n_samples, n_features)\n",
        "\n",
        "y_train: The target values or labels for each sample. It is a 1D array (or vector) containing the corresponding labels.\n",
        "\n",
        "Shape: (n_samples,) (for classification or regression)\n",
        "\n",
        "2.Keras/TensorFlow (model.fit() in Keras/TensorFlow):-In Keras (a high-level API for TensorFlow), model.fit() is used to train a neural network model.\n",
        "\n",
        "→ Common Arguments:\n",
        "x_train: Input training data (features).\n",
        "\n",
        "Shape: (n_samples, n_features)\n",
        "y_train: Target labels corresponding to the training data.\n",
        "\n",
        "Shape: (n_samples,) (for classification) or (n_samples, n_output) (for regression)\n",
        "epochs (optional): The number of times the entire training dataset will be passed through the model during training. Default is 1.\n",
        "\n",
        "Example: epochs=10 will train the model for 10 complete passes through the dataset.\n",
        "batch_size (optional): The number of samples per gradient update. Default is 32.\n",
        "\n",
        "It determines how many samples are processed before the model’s parameters are updated.\n",
        "Example: batch_size=64 means the model will update parameters after processing 64 samples.\n",
        "validation_data (optional): A tuple (x_val, y_val) containing validation data. The model will evaluate the loss and metrics on this data after each epoch to monitor its performance on unseen data.\n",
        "\n",
        "Example: (x_val, y_val) where x_val is validation features and y_val is validation labels.\n",
        "verbose (optional): Integer value controlling the verbosity of the training process. Default is 1 for displaying progress bars.\n",
        "\n",
        "verbose=0: No output.\n",
        "verbose=1: Progress bar.\n",
        "verbose=2: One line per epoch.\n",
        "callbacks (optional): A list of callback functions (like ModelCheckpoint, EarlyStopping, etc.) to be applied during training.\n",
        "\n",
        "shuffle (optional): Boolean (default True), whether to shuffle the training data before each epoch."
      ],
      "metadata": {
        "id": "gRrpc3VXgN76"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Example data\n",
        "X = [[0, 0], [1, 1], [0, 1], [1, 0]]\n",
        "y = [0, 1, 1, 0]\n",
        "\n",
        "# Split data into training and testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "# Initialize model\n",
        "model = LogisticRegression()\n",
        "\n",
        "# Fit the model to the training data\n",
        "model.fit(X_train, y_train)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "id": "7Is3CYOTPsWn",
        "outputId": "1af2e7ec-c834-40f6-da3e-3c42bb66bfa5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression()"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {\n",
              "  /* Definition of color scheme common for light and dark mode */\n",
              "  --sklearn-color-text: black;\n",
              "  --sklearn-color-line: gray;\n",
              "  /* Definition of color scheme for unfitted estimators */\n",
              "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
              "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
              "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
              "  --sklearn-color-unfitted-level-3: chocolate;\n",
              "  /* Definition of color scheme for fitted estimators */\n",
              "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
              "  --sklearn-color-fitted-level-1: #d4ebff;\n",
              "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
              "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
              "\n",
              "  /* Specific color for light theme */\n",
              "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
              "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-icon: #696969;\n",
              "\n",
              "  @media (prefers-color-scheme: dark) {\n",
              "    /* Redefinition of color scheme for dark theme */\n",
              "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
              "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-icon: #878787;\n",
              "  }\n",
              "}\n",
              "\n",
              "#sk-container-id-1 {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 pre {\n",
              "  padding: 0;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-hidden--visually {\n",
              "  border: 0;\n",
              "  clip: rect(1px 1px 1px 1px);\n",
              "  clip: rect(1px, 1px, 1px, 1px);\n",
              "  height: 1px;\n",
              "  margin: -1px;\n",
              "  overflow: hidden;\n",
              "  padding: 0;\n",
              "  position: absolute;\n",
              "  width: 1px;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-dashed-wrapped {\n",
              "  border: 1px dashed var(--sklearn-color-line);\n",
              "  margin: 0 0.4em 0.5em 0.4em;\n",
              "  box-sizing: border-box;\n",
              "  padding-bottom: 0.4em;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-container {\n",
              "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
              "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
              "     so we also need the `!important` here to be able to override the\n",
              "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
              "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
              "  display: inline-block !important;\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-text-repr-fallback {\n",
              "  display: none;\n",
              "}\n",
              "\n",
              "div.sk-parallel-item,\n",
              "div.sk-serial,\n",
              "div.sk-item {\n",
              "  /* draw centered vertical line to link estimators */\n",
              "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
              "  background-size: 2px 100%;\n",
              "  background-repeat: no-repeat;\n",
              "  background-position: center center;\n",
              "}\n",
              "\n",
              "/* Parallel-specific style estimator block */\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item::after {\n",
              "  content: \"\";\n",
              "  width: 100%;\n",
              "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
              "  flex-grow: 1;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel {\n",
              "  display: flex;\n",
              "  align-items: stretch;\n",
              "  justify-content: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
              "  align-self: flex-end;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
              "  align-self: flex-start;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
              "  width: 0;\n",
              "}\n",
              "\n",
              "/* Serial-specific style estimator block */\n",
              "\n",
              "#sk-container-id-1 div.sk-serial {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "  align-items: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  padding-right: 1em;\n",
              "  padding-left: 1em;\n",
              "}\n",
              "\n",
              "\n",
              "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
              "clickable and can be expanded/collapsed.\n",
              "- Pipeline and ColumnTransformer use this feature and define the default style\n",
              "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
              "*/\n",
              "\n",
              "/* Pipeline and ColumnTransformer style (default) */\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable {\n",
              "  /* Default theme specific background. It is overwritten whether we have a\n",
              "  specific estimator or a Pipeline/ColumnTransformer */\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "/* Toggleable label */\n",
              "#sk-container-id-1 label.sk-toggleable__label {\n",
              "  cursor: pointer;\n",
              "  display: block;\n",
              "  width: 100%;\n",
              "  margin-bottom: 0;\n",
              "  padding: 0.5em;\n",
              "  box-sizing: border-box;\n",
              "  text-align: center;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
              "  /* Arrow on the left of the label */\n",
              "  content: \"▸\";\n",
              "  float: left;\n",
              "  margin-right: 0.25em;\n",
              "  color: var(--sklearn-color-icon);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "/* Toggleable content - dropdown */\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content {\n",
              "  max-height: 0;\n",
              "  max-width: 0;\n",
              "  overflow: hidden;\n",
              "  text-align: left;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content pre {\n",
              "  margin: 0.2em;\n",
              "  border-radius: 0.25em;\n",
              "  color: var(--sklearn-color-text);\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
              "  /* Expand drop-down */\n",
              "  max-height: 200px;\n",
              "  max-width: 100%;\n",
              "  overflow: auto;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
              "  content: \"▾\";\n",
              "}\n",
              "\n",
              "/* Pipeline/ColumnTransformer-specific style */\n",
              "\n",
              "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator-specific style */\n",
              "\n",
              "/* Colorize estimator box */\n",
              "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
              "#sk-container-id-1 div.sk-label label {\n",
              "  /* The background is the default theme color */\n",
              "  color: var(--sklearn-color-text-on-default-background);\n",
              "}\n",
              "\n",
              "/* On hover, darken the color of the background */\n",
              "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "/* Label box, darken color on hover, fitted */\n",
              "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator label */\n",
              "\n",
              "#sk-container-id-1 div.sk-label label {\n",
              "  font-family: monospace;\n",
              "  font-weight: bold;\n",
              "  display: inline-block;\n",
              "  line-height: 1.2em;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label-container {\n",
              "  text-align: center;\n",
              "}\n",
              "\n",
              "/* Estimator-specific */\n",
              "#sk-container-id-1 div.sk-estimator {\n",
              "  font-family: monospace;\n",
              "  border: 1px dotted var(--sklearn-color-border-box);\n",
              "  border-radius: 0.25em;\n",
              "  box-sizing: border-box;\n",
              "  margin-bottom: 0.5em;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "/* on hover */\n",
              "#sk-container-id-1 div.sk-estimator:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
              "\n",
              "/* Common style for \"i\" and \"?\" */\n",
              "\n",
              ".sk-estimator-doc-link,\n",
              "a:link.sk-estimator-doc-link,\n",
              "a:visited.sk-estimator-doc-link {\n",
              "  float: right;\n",
              "  font-size: smaller;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1em;\n",
              "  height: 1em;\n",
              "  width: 1em;\n",
              "  text-decoration: none !important;\n",
              "  margin-left: 1ex;\n",
              "  /* unfitted */\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted,\n",
              "a:link.sk-estimator-doc-link.fitted,\n",
              "a:visited.sk-estimator-doc-link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "/* Span, style for the box shown on hovering the info icon */\n",
              ".sk-estimator-doc-link span {\n",
              "  display: none;\n",
              "  z-index: 9999;\n",
              "  position: relative;\n",
              "  font-weight: normal;\n",
              "  right: .2ex;\n",
              "  padding: .5ex;\n",
              "  margin: .5ex;\n",
              "  width: min-content;\n",
              "  min-width: 20ex;\n",
              "  max-width: 50ex;\n",
              "  color: var(--sklearn-color-text);\n",
              "  box-shadow: 2pt 2pt 4pt #999;\n",
              "  /* unfitted */\n",
              "  background: var(--sklearn-color-unfitted-level-0);\n",
              "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted span {\n",
              "  /* fitted */\n",
              "  background: var(--sklearn-color-fitted-level-0);\n",
              "  border: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link:hover span {\n",
              "  display: block;\n",
              "}\n",
              "\n",
              "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link {\n",
              "  float: right;\n",
              "  font-size: 1rem;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1rem;\n",
              "  height: 1rem;\n",
              "  width: 1rem;\n",
              "  text-decoration: none;\n",
              "  /* unfitted */\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "#sk-container-id-1 a.estimator_doc_link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;LogisticRegression<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.linear_model.LogisticRegression.html\">?<span>Documentation for LogisticRegression</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>LogisticRegression()</pre></div> </div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "import numpy as np\n",
        "\n",
        "# Example dataset (4 samples, 2 features)\n",
        "X_train = np.array([[0, 0], [1, 1], [0, 1], [1, 0]])\n",
        "y_train = np.array([0, 1, 1, 0])\n",
        "\n",
        "# Build a simple model\n",
        "model = Sequential([\n",
        "    Dense(10, activation='relu', input_dim=2),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "zJvGQkD2ibhE",
        "outputId": "d9b9eb79-9bdf-4bbc-9fe7-f3d9010a0b99"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.3333 - loss: 0.7531 \n",
            "Epoch 2/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.0000e+00 - loss: 0.7551 \n",
            "Epoch 3/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.0000e+00 - loss: 0.7418 \n",
            "Epoch 4/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.0000e+00 - loss: 0.7315 \n",
            "Epoch 5/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.0000e+00 - loss: 0.7450 \n",
            "Epoch 6/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.0000e+00 - loss: 0.7431 \n",
            "Epoch 7/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.0000e+00 - loss: 0.7336 \n",
            "Epoch 8/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.0000e+00 - loss: 0.7431 \n",
            "Epoch 9/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.0000e+00 - loss: 0.7412 \n",
            "Epoch 10/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.0000e+00 - loss: 0.7393 \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7f1cef870910>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q19. What does model.predict() do? What arguments must be given?\n",
        "\n",
        "### Ans\n",
        "\n",
        "→ The model.predict() function is used to make predictions on new, unseen data using a trained machine learning or deep learning model. Once the model has been trained (using model.fit()), you can use model.predict() to infer the model's output for new inputs.\n",
        "\n",
        "→ What does model.predict() do?\n",
        "→ Ans.Prediction: It uses the learned parameters (weights and biases) from the training process to predict the output for the provided input data.\n",
        "\n",
        "For regression: It outputs continuous values (e.g., predicted numerical values).\n",
        "\n",
        "For classification: It outputs class labels or probabilities for each class (e.g., probabilities in a multi-class classification task or the predicted class in binary classification).\n",
        "\n",
        "→In summary, the model.predict() function computes and returns the model's predicted output for the given input data based on what it learned during training.\n",
        "\n",
        "→ Arguments for model.predict():-The arguments you need to pass to model.predict() depend on the specific framework you're using (e.g., scikit-learn, Keras/TensorFlow, etc.), but the main requirement is generally the input data (X).\n",
        "\n",
        "Here’s a breakdown of common arguments:\n",
        "\n",
        "1. In Scikit-learn:-In scikit-learn, model.predict() is typically used after training a model (such as a classifier or regressor) to predict the output for new data.\n",
        "\n",
        "\n",
        "model.predict(X_test)\n",
        "\n",
        "→ Required Argument:\n",
        "X_test: The input features for which you want to predict the output. This should be a 2D array or matrix (even for a single sample), where each row represents a data point, and each column represents a feature.\n",
        "Shape: (n_samples, n_features).\n",
        "\n",
        "2. In Keras/TensorFlow:-In Keras/TensorFlow, model.predict() is used for making predictions on new data after training a deep learning model. The model could be a neural network, and predictions can be for tasks like classification, regression, etc.\n",
        "\n",
        "model.predict(X_test, batch_size=32, verbose=0)\n",
        "\n",
        "→ Arguments:\n",
        "X_test: Input features (as a 2D array for multi-sample predictions or 1D array for single-sample predictions).\n",
        "\n",
        "Shape: (n_samples, n_features) for a single prediction (e.g., a test set).\n",
        "batch_size (optional): The number of samples to process at a time. Default is 32. This argument is useful when making predictions on large datasets and can optimize memory usage and processing speed. Larger batch sizes can lead to faster predictions, while smaller batch sizes are more memory efficient.\n",
        "\n",
        "Example: batch_size=64.\n",
        "verbose (optional): Controls the display of progress bars. Default is 0 (no output), 1 for a progress bar, and 2 for one line per prediction.\n",
        "\n"
      ],
      "metadata": {
        "id": "vuzkGpweiwWR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "import numpy as np\n",
        "\n",
        "# Example dataset (4 samples, 2 features)\n",
        "X_train = np.array([[0, 0], [1, 1], [0, 1], [1, 0]])\n",
        "y_train = np.array([0, 1, 1, 0])\n",
        "\n",
        "# Build a simple model\n",
        "model = Sequential([\n",
        "    Dense(10, activation='relu', input_dim=2),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=2)\n",
        "\n",
        "# Predict on the new data\n",
        "X_test = np.array([[0, 1], [1, 0]])\n",
        "predictions = model.predict(X_test)\n",
        "print(predictions)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "7csXZStQile_",
        "outputId": "5f40374a-65b7-424a-cef0-28911a148e47"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.6719 \n",
            "Epoch 2/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6667 - loss: 0.6700 \n",
            "Epoch 3/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6667 - loss: 0.6498 \n",
            "Epoch 4/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8333 - loss: 0.6471 \n",
            "Epoch 5/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8333 - loss: 0.6465 \n",
            "Epoch 6/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6667 - loss: 0.6663 \n",
            "Epoch 7/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6667 - loss: 0.6654 \n",
            "Epoch 8/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6667 - loss: 0.6645 \n",
            "Epoch 9/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6667 - loss: 0.6457 \n",
            "Epoch 10/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8333 - loss: 0.6420 \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
            "[[0.5094165 ]\n",
            " [0.43454164]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Example data\n",
        "X = [[0, 0], [1, 1], [0, 1], [1, 0]]\n",
        "y = [0, 1, 1, 0]\n",
        "\n",
        "# Split data into training and testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "# Initialize and train the model\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test data\n",
        "predictions = model.predict(X_test)\n",
        "print(predictions)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "W3avumAKkrRj",
        "outputId": "91035442-27cc-416f-c39c-e0b461c8a046"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q20.What are continuous and categorical variables?\n",
        "\n",
        "### Ans This questions is repeatative so please refred Q5 for this ans"
      ],
      "metadata": {
        "id": "iSKdKEz6k8Mf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q21. What is feature scaling? How does it help in Machine Learning?\n",
        "\n",
        "### Ans\n",
        "\n",
        "→ Feature scaling is a data preprocessing technique in machine learning where all numerical features in a dataset are transformed to a common scale, ensuring that no single feature dominates the model's learning process by having a significantly larger range of values compared to others; this allows for more accurate and efficient model training, especially when using algorithms sensitive to feature magnitudes like KNN, Support Vector Machines, and neural networks.\n",
        "\n",
        "→ Feature scaling is the process of normalizing the range of features in a dataset. Real-world datasets often contain features that are varying in degrees of magnitude, range, and units. Therefore, in order for machine learning models to interpret these features on the same scale, we need to perform feature scaling.\n",
        "\n",
        "→ Why it's important:-When features have different scales, algorithms might prioritize features with larger ranges, leading to biased results.\n",
        "\n",
        "→ How it works:-Feature scaling typically involves either normalizing values to fall within a specific range (e.g., 0 to 1) or standardizing them to have a mean of 0 and a standard deviation of 1.\n",
        "\n",
        "→ Common feature scaling methods:\n",
        "Min-Max scaling: Scales features to a range between 0 and 1\n",
        "\n",
        "Standardization (Z-score normalization): Transforms features to have a mean of 0 and a standard deviation of 1\n",
        "\n",
        "Robust scaling: Uses percentiles to scale data, which can be helpful when dealing with outliers\n",
        "\n",
        "\n",
        "→ When to use feature scaling:\n",
        "\n",
        "1. When using algorithms sensitive to feature magnitudes like KNN, Support Vector Machines, and neural networks\n",
        "\n",
        "2. When features in your dataset have significantly different ranges\n",
        "\n",
        "\n",
        "**Why is Feature Scaling Important in Machine Learning?**\n",
        "\n",
        "Feature scaling is essential for many machine learning algorithms, as it ensures that the model treats all features equally, especially when features have different units or scales. If the features are not scaled, the machine learning algorithm might give more importance to features with larger values or variance, potentially leading to biased or inaccurate models.\n",
        "\n",
        "Here’s why feature scaling is important:-\n",
        "\n",
        "1. Convergence in Gradient-based Algorithms:-In algorithms like gradient descent (used in linear regression, logistic regression, neural networks, etc.), the cost function is optimized iteratively. If features are on different scales, the algorithm may struggle to converge or may take longer to reach the optimal solution because the gradient steps might be uneven in magnitude.\n",
        "Feature scaling ensures that each feature contributes equally to the gradient updates, making convergence faster and more stable.\n",
        "\n",
        "2. Distance-based Algorithms:-Algorithms like K-Nearest Neighbors (KNN) and Support Vector Machines (SVM) rely on the calculation of distances between data points (Euclidean distance or others).\n",
        "If one feature has a much larger scale than another, the algorithm will disproportionately rely on the feature with larger values, ignoring the smaller-scaled features. Feature scaling ensures that all features contribute equally to the distance calculation.\n",
        "\n",
        "3. Regularization:In regularized regression models (e.g., Ridge or Lasso regression), the penalty term depends on the magnitude of the coefficients. If the features are not scaled, some features might dominate the regularization process due to their larger range, making the model less generalizable.\n",
        "Feature scaling helps ensure that the regularization term applies uniformly to all features.\n",
        "\n",
        "4. Improved Performance:For certain models, such as neural networks, scaling the features can lead to better performance. When features are scaled appropriately, the optimization process (such as backpropagation) becomes more efficient, leading to a better solution with fewer iterations.\n"
      ],
      "metadata": {
        "id": "d42VlvcSlcPj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q22. How do we perform scaling in Python?\n",
        "\n",
        "### Ans\n",
        "\n",
        "→ Common feature scaling methods:\n",
        "\n",
        "1.  Min-Max scaling:- Scales features to a range between 0 and 1\n",
        "\n",
        "2. Standardization (Z-score normalization):- Transforms features to have a mean of 0 and a standard deviation of 1\n",
        "\n",
        "3. Robust scaling:- Uses percentiles to scale data, which can be helpful when dealing with outliers\n",
        "\n",
        "→ lets see in example below each scaling method"
      ],
      "metadata": {
        "id": "ZH1NcISin07B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Example data (3 samples, 2 features)\n",
        "X = [[1, 2], [3, 4], [5, 6]]\n",
        "\n",
        "# Initialize the MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# Fit and transform the data\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "print(\"Scaled data (Min-Max):\\n\", X_scaled)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "_RzEXMu3k0WO",
        "outputId": "1a74249c-67b4-4bdd-e218-00648542b89a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scaled data (Min-Max):\n",
            " [[0.  0. ]\n",
            " [0.5 0.5]\n",
            " [1.  1. ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Example data (3 samples, 2 features)\n",
        "X = [[1, 2], [3, 4], [5, 6]]\n",
        "\n",
        "# Initialize the StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit and transform the data\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "print(\"Scaled data (Standardization):\\n\", X_scaled)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "B1duzo5vouTl",
        "outputId": "87e8d82a-6446-4058-c724-05d932c10b3c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scaled data (Standardization):\n",
            " [[-1.22474487 -1.22474487]\n",
            " [ 0.          0.        ]\n",
            " [ 1.22474487  1.22474487]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import RobustScaler\n",
        "\n",
        "# Example data (3 samples, 2 features)\n",
        "X = [[1, 2], [3, 4], [5, 100]]\n",
        "\n",
        "# Initialize the RobustScaler\n",
        "scaler = RobustScaler()\n",
        "\n",
        "# Fit and transform the data\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "print(\"Scaled data (Robust Scaling):\\n\", X_scaled)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "RlTCPsKWoyF3",
        "outputId": "19d485fa-8d1e-4bdf-df20-727f8e8aa985"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scaled data (Robust Scaling):\n",
            " [[-1.         -0.04081633]\n",
            " [ 0.          0.        ]\n",
            " [ 1.          1.95918367]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q23. What is sklearn.preprocessing?\n",
        "## Q24. How do we split data for model fitting (training and testing) in Python?\n",
        "\n",
        "### Ans This questions is repeatative so please refred Q8 and Q10 for thir ans\n"
      ],
      "metadata": {
        "id": "6PbIFoYjo-pD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q25. Explain data encoding?\n",
        "\n",
        "### Ans\n",
        "\n",
        "\n",
        "→ Data encoding:- Data encoding is the process of converting categorical data (non-numeric) into a numerical format that machine learning algorithms can understand. Since many machine learning models (like linear regression, decision trees, and neural networks) require numerical input, data encoding is essential for preprocessing categorical features.\n",
        "\n",
        "→ Why is Data Encoding Important?\n",
        "Categorical data, such as \"Red\", \"Green\", \"Blue\" for a color feature, or \"Male\", \"Female\" for gender, cannot be directly processed by most machine learning algorithms, which expect numeric values. Therefore, encoding is used to transform these categorical variables into a format that models can interpret while preserving the underlying information.\n",
        "\n",
        "→ Common Types of Data Encoding\n",
        "There are several methods for encoding categorical data, with the most common being:\n",
        "\n",
        "1. Label Encoding\n",
        "2. One-Hot Encoding\n",
        "3. Ordinal Encoding\n",
        "4. Binary Encoding\n",
        "5. Frequency Encoding"
      ],
      "metadata": {
        "id": "8kOXUuQopoWm"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AE_T4tVwo1XF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}